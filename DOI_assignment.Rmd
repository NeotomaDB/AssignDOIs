---
title: "DOI workflow"
author: "Simon Goring"
date: "May 22, 2016"
output:
  html_document:
    code_folding: show
    highlight: pygment
    number_sections: yes
    theme: journal
    toc: yes
    toc_depth: 3
  pdf_document:
    toc: yes
    toc_depth: '3'
csl: elsevier-harvard.csl
bibliography: goringetal_references.bib
---

```{r, echo=FALSE, message = FALSE, warnings=FALSE, tidy=TRUE}
library(RMySQL, quietly = TRUE, verbose = FALSE)
library(httr, quietly = TRUE, verbose = FALSE)
library(XML, quietly = TRUE, verbose = FALSE)

source('sql_calls.R')

schema <- xmlSchemaParse('data/metadata.xsd')

con <- dbConnect(MySQL(),
                 user = 'root', 
                 dbname = 'neotoma', 
                 password = 'c@mpf1re', 
                 host = 'localhost')
```

# 120 Character Summary

**Using the Neotoma Paleoecological Database as a case study for improving discoverability through DOIs.**

\pagebreak

# Title Page

\pagebreak

# Abstract

[250 words]

Key Words: metadata, discoverability, dois, data publication, digital curation

# Main Text

## Introduction

Digital object identifier (DOI) names for data provide a critical resource that ties the intellectual contribution of an individual or team of individuals to derived products beyond the life cycle of the publication.  Discoverability of data relies on clear metadata standards and semantic something to facilitate data discovery beyond a single discipline, ensuring that the rewards for parcticing science in an open and transparent manner are fully realized.  Open data and broad data access have become the focus of much attention, both in an effort to facilitate 

The core of the Neotoma Paleoecological Database rests on two fundamental data contributions.  The first, COHMAP [ref], was the result of an international, interdisciplinary effort that resulted in the contribution of XXX datasets across XXX publications from XXX allied researchers.  This research contribution led to one of the first model-data intercomparison projects, with a lasting legacy for paleoclimate and climate modeling. The second large data contribution was the result of the FAUNMAP project, centered on assembling fossil records from the United States spanning the Pleistocene.  FAUNMAP was the result of contributions representing XXX researchers, across XXX unique publications, resulting in fundamental contributions to our understanding of species responses to climate change over long time scales.  In both cases the call for open and unrestricted data sharing was less stringent than it is currently, with fewer enforcement or regulatory elements, but the success of these efforts reflects the enormous value of open data.

Currently, Neotoma contains XXX datasets [as of X].  There are XXX unique data types, and these data products represent the work of XXX individual researchers.  Access to Neotoma is made primarily through three platforms.  The database itself is a SQL Server database [Microsoft] that is available as snapshots from the Neotoma website -- [http://neotomadb.org]() -- while data search capacity is supported through a map-based web engine (Neotoma explorer) or through the API and associated R package `neotoma` [].  The datasbase itself is described more fully elsewhere.

Here we describe the efforts to facilitate the minting and provisioning of DOI names for the Neotoma Database through DataCite/EZID, the steps we have taken to ensure increased discoverability of data, the steps to support ongoing versioning of datasets, and, ultimately, the possibilities that DOIs provide both to Neotoma, to facilitate ongoing database development and evolution, and also to individuals who provide data to Neotoma as part of their data management planning.

## Background

In 2016, Neotoma, through the University of Wisconsin Library System obtained authorization to mint DOI names for data contained within the database.  The DOI system originated through a collaboration among three large publishing associations in 1997 [@doihandbook], and, although it was primarily focused on minting unique identifiers for published materials, the vision of DOI names for data came soon afterwards [@paskin2005digital].  While DOI names do not resolve to the data categorically, they provide metadata (supplied in the process of minting a DOI) and a resolution service that facilitates the long term discoverability and sustainability of data generated by researchers.

In the case of Neotoma the use of DOI names for individual datasets facilitates serveral key objectives:

1.  It allows Neotoma to act as a repository for journals following JDAP policy where proof of data archiving often requires the provisioning of a DOI name.  This provides additional incentives to researchers contributing data to Neotoma as part of a sustainable Data Management plan.

2.  It connects Neotoma data to a broader set of services, in this case facilitated through the EZID and DataCite metadata search engines.  This help connect datasets to publications through additional DOI system metadata, and to unique researchers through services such as ORCID.  This rovides added value to researchers contributing data to Neotoma as it allows a full and open accounting of research contributions, particularly valuable for early career researchers or researchers involved in interdisciplinary research [@goring2014improving], or international collaboration []

3.  It provides a unique and persistent identifer to datasets and their associated metadata that can be resolved from any browser connected to the internet, whether or not the user has knowledge of Neotoma and its specific database location, schema or API.

To provision DOI names for datasets 

# Setup:

```
# Required libraries:
library(RMySQL, quietly = TRUE, verbose = FALSE)
library(httr, quietly = TRUE, verbose = FALSE)
library(XML, quietly = TRUE, verbose = FALSE)

# The stored SQL calls are in a seperate file:
source('sql_calls.R')

con <- dbConnect(MySQL(),
                 user = 'root', 
                 dbname = ***************, 
                 password = ***************, 
                 host = 'localhost')

# This is the schema we're checking against:
schema <- xmlSchemaParse('data/metadata.xsd')
```

Setup is fairly straightforward.  SQL Server can use R in Stored Procedures -- [link here](https://msdn.microsoft.com/en-ca/library/mt604845.aspx) -- so I've revised my code because it was ultimately easier for me to do this in R.  This whole procedure can run off of the Neotoma Server as an embedded component within the database.

The script for the SQL commands is in a seperate file, but I have posted it to a GitHub [gist](https://gist.github.com/SimonGoring/defa8bcfd2216f80913afebfa29a901a) if you're interested.

# Use Case

Currently the Neotoma Paleoecological Database -- [http://neotomadb.org]() -- has no established system for minting DOIs for records within the database.  This document lays out the outline for establishing DOIs for the database.  We will be using the EZID API and the DataCite metadata schema -- [pdf here](http://schema.datacite.org/meta/kernel-3/doc/DataCite-MetadataKernel_v3.1.pdf) -- as the basis of our procedure.

Given the database structure, discussed in detail [here](http://neotoma-manual.readthedocs.io/en/latest/), there is no natural single DB table that matches up to the DataCite schema, but there is a data construct that mirrors (to some degree) the concept of a complete dataset, the existing Neotoma API `downloads` structure (*e.g.*, [http://api.neotomadb.org/v1/data/downloads/14196]()).  This object will be the atomic unit for DOI assignment and we will refer to it as a "dataset" in this document, although a second API service exists that refers to "datasets". The API "dataset" service excludes count or sample data for a site, and as such doesn't serve our needs for the assignment of DOIs.

Neotoma, in establishing a DOI service, has two main use cases.  Pre-existing records and records that will be provided to Neotoma as a set of datasets (either the inclusion of a new Constituent Database, or as the result of a data synthesis project) will require the bulk assignment records, we refer to this as **"Serial Assignment"**. The second use case occurs when new records are uploaded individually through Tilia ([http://tiliait.com]()) or a data upload web service (in development), we will refer to these as **"Unique Assignment"**.  For each of these two use cases the process is generally the same for the actual call to DataCite/EZID, but the wrapping within the workflow will differ.

## Serial Assignment

Serial assignment of DOIs occurs when we first assign DOIs to all Neotoma records, and when a new constituent database, or large sets of datasets are uploaded to Neotoma.  This process is (likely) triggered manually, or it could be applied to a set of unique dataset IDs.  This process could also work by simply polling Neotoma each day for `Datasets` without DOI assignments, but we want to be able to provide the DOI quickly when people submit their records.

The process is:

* Data are added to Neotoma (with empty dataset DOI field)
* The script run through each dataset with an unassigned DOI
* A landing page is created programmatically.
* The script calls out to DataCite/EZID & generates a DOI and assigns metadata
* The DOI returned from DataCite is attached to the `Datasets` table
* An email is sent to the Dataset PI with the dataset DOI & metadata, and to the relevant steward - **this system needs to be set up as well**

## Unique Assignment

Unique assignment of a DOI occurs at data upload whether from Tilia, or directly into Neotoma through another portal.

The process is:

* Data are added to Neotoma (with empty dataset DOI field)
* A landing page is created programmatically.
* The script calls out to DataCite/EZID & generates a DOI and assigns metadata
* The DOI returned from DataCite is attached to the `Datasets` table
* Either:
  * The DOI is attached to the "Success" object and returned as part of the process
  * Or, the application polls Neotoma again with the new dataset ID and recieves the DOI.
* An email is sent to the Dataset PI and to the steward.

# Implementation

## Building the XML Document:

To fill in the metadata fields we need to generate a few separate calls.  Here we will use a dataset ID of `1001`, a record for Hail Lake, YT, Canada, generated by Les Cwynar.  [Explorer link](http://apps.neotomadb.org/explorer/?datasetid=1001), [JSON download](http://api.neotomadb.org/v1/data/downloads/1001).

```{r, results='hide', tidy = TRUE}
# Building the XML document:

ds_id <- 1001

# Generating the new XML framework and associated namespaces:
doc <- newXMLDoc()

root <- newXMLNode('resource', 
                   namespaceDefinitions = c("http://datacite.org/schema/kernel-3",
                             "xsi" = "http://www.w3.org/2001/XMLSchema-instance"),
                   attrs = c("xsi:schemaLocation" = "http://datacite.org/schema/kernel-3 http://schema.datacite.org/meta/kernel-3/metadata.xsd"), 
                   doc = doc)

```

## Assigning Metadata Tags

I'm trying to build this documentation in the same order as the [DataCite Schema Documentation](https://schema.datacite.org/meta/kernel-3/doc/DataCite-MetadataKernel_v3.1.pdf) is laid out so that we can follow along.  I've wrapped the SQL queries into R functions so we can push them out into a broader function, one that uses the dataset number (`1001` here) as the key parameter.

### Identifier

First we need to assign an `identifier`, which will be the dataset's Landing Page.  The generation of the landing page is covered elsewhere.

```{r, results = 'hide', message = FALSE, warning = FALSE}
default <- dbGetQuery(con, statement = default_call(ds_id))

# Clean the affiliation:
default$affiliation <- gsub('\r\n', ', ', default$affiliation)

# This is the empty shoulder for assigning DOIs:
newXMLNode("identifier", '10.5072/FK2', 
           attrs = c('identifierType' = 'DOI'), parent = root)

# This creator stuff is just done one at a time. . . 
```

### Creators:

```{r, results = 'hide', message = FALSE, warning = FALSE}
newXMLNode("creators", parent = root)

lapply(1:nrow(default),
       function(x) {
         addChildren(root[["creators"]], newXMLNode("creator",
                    .children = list(newXMLNode("creatorName",
                                                default$creatorName[x]),
                                     newXMLNode("affiliation",
                                                default$affiliation[x]))))
        })

```

### Titles
```{r, results = 'hide', message = FALSE, warning = FALSE}
newXMLNode("titles", parent = root)
newXMLNode("title", 
           default$SiteName[1],
           attrs = c("xml:lang" = "en-us"),
           parent = root[["titles"]])

```

### Publisher & Publication Year

Are we considering the publication year the year that this data has gone up to DataCite (e.g., is pushing for a DOI the publication?) or is it the year it was first submitted to Neotoma? (see Dates below)

```{r, results = 'hide', message = FALSE, warning = FALSE}
newXMLNode("publisher", "Neotoma Paleoecological Database", parent = root)

# Number 5:
newXMLNode("publicationYear", format(Sys.Date(), "%Y"), parent = root)
```

### Subject

The Subject and Subject Scheme information could use some work & reflection.  I've created a document [here](https://docs.google.com/document/d/1wcR6WWAV3COD_MeOgaOEqtpJ3bbe1nxF9omnmmlEA5Q/edit?usp=sharing) that discusses it in more detail..  For now I will assign everything "Paleoecology".

```{r, results = 'hide', message = FALSE, warning = FALSE}

newXMLNode("subjects", newXMLNode("subject", 
                                  "Paleoecology",
                                  attrs = c("subjectScheme" = "Library of Congress",
                                            "schemeURI" = "http://id.loc.gov/authorities/subjects")), 
           parent = root)

```

### Contributors

The contributor information comes from the Contact tables for the record.  There is a query (in `sql_calls.R`) that pulls in contributor information.  Right now we are not tagging the name identifiers, but we could do this if we want (although it's not supported in the database).  **A question for DataCite**: is it's okay to have people listed with multiple roles, and what does a "co-author" get tagged as.

```{r results='as-is', echo=FALSE}
contacts <- dbGetQuery(con, statement = contributor_call(ds_id))

contacts$affiliation <- gsub('\r\n', ', ', contacts$affiliation)
knitr::kable(contacts)
```

```{r, results='hide'}

# The contributors come from the DB call.  This duplicates 
newXMLNode("contributors", parent = root)
lapply(1:nrow(contacts), 
       function(x) { 
         newXMLNode("contributor", 
                    attrs = c("contributorType" = contacts$contributorType[x]),
                    .children = list(newXMLNode("contributorName", 
                                                contacts$creatorName[x])), parent = root[["contributors"]])
         })
```

### Dates
Adding the date fields uses an `lapply` comment on a date query (found in the SQL query in `sql_calls.R`).

```{r results='as-is', echo = FALSE}
dates   <- dbGetQuery(con, statement = date_call(ds_id))
knitr::kable(dates)
```

```{r, results='hide'}
# Adding the dates in one at a time, we use the lapply to insert them
# into the `dates` node.
newXMLNode("dates", parent = root)
lapply(1:nrow(dates), 
       function(x) { 
         newXMLNode("date", 
                    format(as.Date(dates[1,1]), "%Y-%m-%d"), 
                           attrs = c("dateType" = dates[x,2]), 
                    parent = root[["dates"]]) 
         } )

```

### Language and Resource Type

```{r, echo = TRUE, results='hide'}
# Number 9:
newXMLNode("language", "English", parent = root)

# Number 10:
newXMLNode("resourceType", "Dataset/Paleoecological Sample Data", 
           attrs = c("resourceTypeGeneral" = "Dataset"), parent = root)

```

I haven't stuck in the alternate identifiers, but we can add these in.  Most likely this will be sample IGSNs, &cetera.  We can figure this out at some point.

### Related Identifiers

`RelatedIdentifiers` will point to the URL of the static landing page.  It will also point to the publications relating to the dataset, but it's not clear what the appropriate `relationType` is here.  Right now pointing to the API is enabled (I'm also not sure which `relationType` is appropriate here), but I've left the static page commented out.  If we have the related Pangea data URI somewhere then we need to add that in as well.

```{r, results = 'hide', message = FALSE, warning = FALSE}

newXMLNode("relatedIdentifiers", parent = root)

newXMLNode("relatedIdentifier", paste0("api.neotomadb.org/v1/downloads/", ds_id),
  attrs = list(relationType = "IsMetadataFor",
               relatedIdentifierType = "URL",
               relatedMetadataScheme = "json"),
  parent = root[["relatedIdentifiers"]])

# Add DOI tags:
dois <- dbGetQuery(con, statement = doi_call(ds_id))

if (is.na(dois)) {
  # There's no current DOI
} else {
  lapply(unlist(dois), function(x){
    newXMLNode("relatedIdentifier", paste0("doi:", x),
  attrs = list(relationType = "IsDocumentedBy",
               relatedIdentifierType = "DOI"),
  parent = root[["relatedIdentifiers"]])
  })
}

# Not implemented: Add Static page IDs.
#newXMLNode("relatedIdentifier", paste0("http://sites.neotomadb.org/", dataset),
#  attrs = list(relatedIdentifierType="URL"),
#  parent = root[["relatedIdentifiers"]])
```

### Size

Fun!  I just use the API to get the record & check out its size.

```{r, results='hide'}
# Number 13: size
size <- as.numeric(object.size(GET(paste0("api.neotomadb.org/v1/downloads/", ds_id))))
newXMLNode("sizes", newXMLNode("size", paste0(ceiling(size/1000), " KB")), parent = root)
```

### Format

So, right now I am going to assign three formats, assuming that the data will be available in each of the three.

```{r, results = 'hide', message = FALSE, warning = FALSE}
# Number 14:
newXMLNode("formats", parent = root)
newXMLNode("format", "XML", parent = root[["formats"]])
newXMLNode("format", "TLX", parent = root[["formats"]])
newXMLNode("format", "JSON", parent = root[["formats"]])
```

### Version

`Version` Needs to be implemented.  What is our decision?

### Rights List

Here I'm just pulling right from the Neotoma Data Use agreement, but note that ultimately the data itself isn't protected from copyright, only the database structure.  So I'm not actually sure what we can do here.

```{r, results='hide'}
# Number 16

addChildren(newXMLNode("rightsList", parent = root),
           children = newXMLNode("rights", "CC-BY4", 
           attrs = c("rightsURI" = "http://creativecommons.org/licenses/by/4.0/deed.en_US")))

```

### Description

`Description` and `DescriptionType`, not sure what to put here.  It's supposed to be technical information & best practice to supply a Description.  Do we use site description, something else?

### GeoLocation

I use the full four coordinate string here.  People can figure out if it's a polygon or not.

```{r, results = 'hide', message = FALSE, warning = FALSE}
loc <- dbGetQuery(con, statement = geoloc_call(1001))

newXMLNode("geoLocations", parent = root)
newXMLNode("geoLocation", 
           newXMLNode("geoLocationBox", loc, parent = root),
           parent = root[["geoLocations"]])

```


### Validation and Upload

```{r, message = FALSE, warning = FALSE}
xmlSchemaValidate('data/metadata.xsd', doc)
```

The schema now validates properly, we're happy about that, and so we push it to DataCite:

```{r, results = 'hide', message = FALSE, warning = FALSE}

urlbase = 'https://ezid.cdlib.org/'

# We need to clean up the XML formatting, removing hard returns and changing quotes:
parse_doc <- gsub('\\n', '', paste0('datacite: ',
                         saveXML(xmlParse(paste0('data/datasets/', 
                                                 ds_id, '_output.xml')))))
parse_doc <- gsub('\\"', "'", parse_doc)

r = POST(url = paste0(urlbase, 'shoulder/doi:10.5072/FK2'), 
	       authenticate(user = 'apitest', password = 'apitest'),
         add_headers(c('Content-Type' = 'text/plain; charset=UTF-8',
                       'Accept' = 'text/plain')),
	       body = parse_doc,
	                   format = 'xml')

out_doi <- substr(content(r), 
                  regexpr("doi:", content(r)), 
                  regexpr("\\s\\|", content(r)) - 1)

# Now that we've got an assigned DOI, push it into the XML:
```

This then gives us a DOI `r out_doi` that we can check.  In the current implementation we'll need to update the record with the new identifier, but once we get our own shoulder we'll be fine and won't have to update `identifier` after data upload.

## Updating Records

In cases where a data set is updated, Neotoma still directly modifies the record.  This will (or may) change.  Now that we can build the XML okay we should be able to modify the DOI metadata using:

```{r, results = 'hide', message = FALSE, warning = FALSE}

xmlValue(root[["identifier"]]) <- out_doi

saveXML(doc = doc, 
        file = paste0('data/datasets/', ds_id, '_output.xml'),
        prefix = '<?xml version="1.0" encoding="UTF-8"?>')

r = POST(url = paste0(urlbase, paste0('identifier/', out_doi)), 
	       authenticate(user = 'apitest', password = 'apitest'),
         add_headers(c('Content-Type' = 'text/plain; charset=UTF-8',
                       'Accept' = 'text/plain')),
	       body = paste0('datacite: ',paste(readLines(paste0('data/datasets/', 
                ds_id,  '_output.xml')), collapse = " ")),
	                   format = 'xml')
content(r)
```

This is what DataCite has to say about Dynamic data:

**For datasets that are continuously and rapidly updated, there are special challenges both in citation and preservation. For citation, three approaches are possible:**
  
  *1. Cite a specific slice (the set of updates to the dataset made during a particular period of time or to a particular area of the dataset);*
  
  *2. Cite a specific snap shot (a copy of the entire dataset made at a specific time);*
  
  *3. Cite the continuously updated dataset, but add an Access Date and Time to the citation.*
  
**Note that a “slice” and “snap shot” are versions of the dataset and require unique identifiers. The third option is controversial, because it necessarily means that following the citation does not result in observation of the resource as cited.**

## Conclusion

We now have a fully realized implementation of the process required to mint a DOI (and to extract the relevant data from the Neotoma Database).  This needs to be turned into a function, which should be fairly straightforward.  The key issue is passing the dataset ID and making sure it's all going to work.  We also need to keep track of the dataset IDs and which ones work and which ones don't.

# Acknowledgements

The authors would like to acknowledge the support of the University of Wisconsin Library System, particularly Brianna Marshall, Chair of Research Data Services.  

# Competing Interests

# References